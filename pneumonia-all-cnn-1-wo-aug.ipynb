{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68649a67",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-20T10:11:33.806069Z",
     "iopub.status.busy": "2025-07-20T10:11:33.805769Z",
     "iopub.status.idle": "2025-07-20T11:47:46.769669Z",
     "shell.execute_reply": "2025-07-20T11:47:46.768801Z"
    },
    "papermill": {
     "duration": 5772.986058,
     "end_time": "2025-07-20T11:47:46.788948",
     "exception": false,
     "start_time": "2025-07-20T10:11:33.802890",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-20 10:11:36.815909: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1753006297.012685      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1753006297.069608      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "I0000 00:00:1753006309.998952      19 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m83683744/83683744\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m58889256/58889256\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m80134624/80134624\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m94765736/94765736\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m94668760/94668760\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet101_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m171446536/171446536\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet101v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m171317808/171317808\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet152_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m234698864/234698864\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet152v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m234545216/234545216\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m87910968/87910968\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_resnet_v2/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m219055592/219055592\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet/mobilenet_1_0_224_tf_no_top.h5\n",
      "\u001b[1m17225924/17225924\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
      "\u001b[1m9406464/9406464\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m29084464/29084464\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet169_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m51877672/51877672\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet201_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m74836368/74836368\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/nasnet/NASNet-mobile-no-top.h5\n",
      "\u001b[1m19993432/19993432\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
      "\n",
      "ðŸ”§ Training model: Xception\n",
      "Found 5216 images belonging to 2 classes.\n",
      "Found 624 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1753006369.784366      57 service.cc:148] XLA service 0x7c7068003ca0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1753006369.785399      57 service.cc:156]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
      "I0000 00:00:1753006370.727537      57 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "I0000 00:00:1753006375.363398      57 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163/163 - 98s - 599ms/step - accuracy: 0.9390 - loss: 0.1533 - val_accuracy: 0.8061 - val_loss: 0.5849 - learning_rate: 0.0010\n",
      "Epoch 2/64\n",
      "163/163 - 50s - 308ms/step - accuracy: 0.9519 - loss: 0.1218 - val_accuracy: 0.8013 - val_loss: 0.6317 - learning_rate: 0.0010\n",
      "Epoch 3/64\n",
      "163/163 - 49s - 303ms/step - accuracy: 0.9615 - loss: 0.0969 - val_accuracy: 0.8029 - val_loss: 0.6645 - learning_rate: 0.0010\n",
      "Epoch 4/64\n",
      "163/163 - 50s - 304ms/step - accuracy: 0.9753 - loss: 0.0666 - val_accuracy: 0.8109 - val_loss: 0.6261 - learning_rate: 2.0000e-04\n",
      "\n",
      "ðŸ”§ Training model: VGG16\n",
      "Found 5216 images belonging to 2 classes.\n",
      "Found 624 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/64\n",
      "163/163 - 68s - 415ms/step - accuracy: 0.9270 - loss: 0.2160 - val_accuracy: 0.7548 - val_loss: 0.7828 - learning_rate: 0.0010\n",
      "Epoch 2/64\n",
      "163/163 - 50s - 309ms/step - accuracy: 0.9651 - loss: 0.0928 - val_accuracy: 0.7772 - val_loss: 0.7328 - learning_rate: 0.0010\n",
      "Epoch 3/64\n",
      "163/163 - 51s - 312ms/step - accuracy: 0.9774 - loss: 0.0607 - val_accuracy: 0.8221 - val_loss: 0.6745 - learning_rate: 0.0010\n",
      "Epoch 4/64\n",
      "163/163 - 50s - 309ms/step - accuracy: 0.9801 - loss: 0.0511 - val_accuracy: 0.7003 - val_loss: 1.5955 - learning_rate: 0.0010\n",
      "Epoch 5/64\n",
      "163/163 - 51s - 311ms/step - accuracy: 0.9820 - loss: 0.0452 - val_accuracy: 0.8542 - val_loss: 0.5324 - learning_rate: 0.0010\n",
      "Epoch 6/64\n",
      "163/163 - 50s - 310ms/step - accuracy: 0.9881 - loss: 0.0314 - val_accuracy: 0.8013 - val_loss: 0.8686 - learning_rate: 0.0010\n",
      "Epoch 7/64\n",
      "163/163 - 51s - 311ms/step - accuracy: 0.9873 - loss: 0.0351 - val_accuracy: 0.7612 - val_loss: 1.1577 - learning_rate: 0.0010\n",
      "Epoch 8/64\n",
      "163/163 - 51s - 314ms/step - accuracy: 0.9962 - loss: 0.0162 - val_accuracy: 0.7869 - val_loss: 0.9880 - learning_rate: 2.0000e-04\n",
      "\n",
      "ðŸ”§ Training model: VGG19\n",
      "Found 5216 images belonging to 2 classes.\n",
      "Found 624 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/64\n",
      "163/163 - 56s - 343ms/step - accuracy: 0.9498 - loss: 0.1362 - val_accuracy: 0.8109 - val_loss: 0.5231 - learning_rate: 0.0010\n",
      "Epoch 2/64\n",
      "163/163 - 51s - 314ms/step - accuracy: 0.9737 - loss: 0.0711 - val_accuracy: 0.7997 - val_loss: 0.7358 - learning_rate: 0.0010\n",
      "Epoch 3/64\n",
      "163/163 - 51s - 314ms/step - accuracy: 0.9827 - loss: 0.0469 - val_accuracy: 0.8349 - val_loss: 0.5706 - learning_rate: 0.0010\n",
      "Epoch 4/64\n",
      "163/163 - 52s - 320ms/step - accuracy: 0.9887 - loss: 0.0285 - val_accuracy: 0.7788 - val_loss: 0.8933 - learning_rate: 2.0000e-04\n",
      "\n",
      "ðŸ”§ Training model: ResNet50\n",
      "Found 5216 images belonging to 2 classes.\n",
      "Found 624 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/64\n",
      "163/163 - 74s - 456ms/step - accuracy: 0.9597 - loss: 0.1049 - val_accuracy: 0.7308 - val_loss: 1.1667 - learning_rate: 0.0010\n",
      "Epoch 2/64\n",
      "163/163 - 50s - 310ms/step - accuracy: 0.9747 - loss: 0.0726 - val_accuracy: 0.7580 - val_loss: 1.0711 - learning_rate: 0.0010\n",
      "Epoch 3/64\n",
      "163/163 - 51s - 313ms/step - accuracy: 0.9847 - loss: 0.0410 - val_accuracy: 0.8478 - val_loss: 0.5295 - learning_rate: 0.0010\n",
      "Epoch 4/64\n",
      "163/163 - 51s - 311ms/step - accuracy: 0.9864 - loss: 0.0389 - val_accuracy: 0.8173 - val_loss: 0.7034 - learning_rate: 0.0010\n",
      "Epoch 5/64\n",
      "163/163 - 51s - 312ms/step - accuracy: 0.9778 - loss: 0.0624 - val_accuracy: 0.8045 - val_loss: 0.8037 - learning_rate: 0.0010\n",
      "Epoch 6/64\n",
      "163/163 - 51s - 311ms/step - accuracy: 0.9933 - loss: 0.0192 - val_accuracy: 0.7804 - val_loss: 1.1101 - learning_rate: 2.0000e-04\n",
      "\n",
      "ðŸ”§ Training model: ResNet50V2\n",
      "Found 5216 images belonging to 2 classes.\n",
      "Found 624 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/64\n",
      "163/163 - 70s - 428ms/step - accuracy: 0.9429 - loss: 0.1455 - val_accuracy: 0.7596 - val_loss: 0.8074 - learning_rate: 0.0010\n",
      "Epoch 2/64\n",
      "163/163 - 49s - 302ms/step - accuracy: 0.9753 - loss: 0.0739 - val_accuracy: 0.8317 - val_loss: 0.5722 - learning_rate: 0.0010\n",
      "Epoch 3/64\n",
      "163/163 - 49s - 299ms/step - accuracy: 0.9783 - loss: 0.0560 - val_accuracy: 0.8686 - val_loss: 0.4375 - learning_rate: 0.0010\n",
      "Epoch 4/64\n",
      "163/163 - 50s - 306ms/step - accuracy: 0.9827 - loss: 0.0466 - val_accuracy: 0.8397 - val_loss: 0.6139 - learning_rate: 0.0010\n",
      "Epoch 5/64\n",
      "163/163 - 50s - 307ms/step - accuracy: 0.9856 - loss: 0.0384 - val_accuracy: 0.8333 - val_loss: 0.6761 - learning_rate: 0.0010\n",
      "Epoch 6/64\n",
      "163/163 - 50s - 307ms/step - accuracy: 0.9941 - loss: 0.0207 - val_accuracy: 0.8221 - val_loss: 0.7225 - learning_rate: 2.0000e-04\n",
      "\n",
      "ðŸ”§ Training model: ResNet101\n",
      "Found 5216 images belonging to 2 classes.\n",
      "Found 624 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/64\n",
      "163/163 - 87s - 537ms/step - accuracy: 0.9567 - loss: 0.1126 - val_accuracy: 0.7372 - val_loss: 1.0690 - learning_rate: 0.0010\n",
      "Epoch 2/64\n",
      "163/163 - 52s - 321ms/step - accuracy: 0.9724 - loss: 0.0737 - val_accuracy: 0.8061 - val_loss: 0.7462 - learning_rate: 0.0010\n",
      "Epoch 3/64\n",
      "163/163 - 52s - 318ms/step - accuracy: 0.9824 - loss: 0.0459 - val_accuracy: 0.7821 - val_loss: 0.8614 - learning_rate: 0.0010\n",
      "Epoch 4/64\n",
      "163/163 - 52s - 321ms/step - accuracy: 0.9893 - loss: 0.0308 - val_accuracy: 0.7692 - val_loss: 0.9789 - learning_rate: 0.0010\n",
      "Epoch 5/64\n",
      "163/163 - 52s - 317ms/step - accuracy: 0.9952 - loss: 0.0175 - val_accuracy: 0.7724 - val_loss: 1.0031 - learning_rate: 2.0000e-04\n",
      "\n",
      "ðŸ”§ Training model: ResNet101V2\n",
      "Found 5216 images belonging to 2 classes.\n",
      "Found 624 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/64\n",
      "163/163 - 81s - 500ms/step - accuracy: 0.9354 - loss: 0.1692 - val_accuracy: 0.8510 - val_loss: 0.4111 - learning_rate: 0.0010\n",
      "Epoch 2/64\n",
      "163/163 - 50s - 308ms/step - accuracy: 0.9695 - loss: 0.0794 - val_accuracy: 0.8093 - val_loss: 0.6127 - learning_rate: 0.0010\n",
      "Epoch 3/64\n",
      "163/163 - 50s - 306ms/step - accuracy: 0.9766 - loss: 0.0663 - val_accuracy: 0.8061 - val_loss: 0.6842 - learning_rate: 0.0010\n",
      "Epoch 4/64\n",
      "163/163 - 50s - 308ms/step - accuracy: 0.9885 - loss: 0.0374 - val_accuracy: 0.7997 - val_loss: 0.7144 - learning_rate: 2.0000e-04\n",
      "\n",
      "ðŸ”§ Training model: ResNet152\n",
      "Found 5216 images belonging to 2 classes.\n",
      "Found 624 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/64\n",
      "163/163 - 100s - 615ms/step - accuracy: 0.9411 - loss: 0.1594 - val_accuracy: 0.8798 - val_loss: 0.3104 - learning_rate: 0.0010\n",
      "Epoch 2/64\n",
      "163/163 - 52s - 321ms/step - accuracy: 0.9691 - loss: 0.0816 - val_accuracy: 0.8397 - val_loss: 0.4731 - learning_rate: 0.0010\n",
      "Epoch 3/64\n",
      "163/163 - 52s - 320ms/step - accuracy: 0.9814 - loss: 0.0533 - val_accuracy: 0.7804 - val_loss: 0.8194 - learning_rate: 0.0010\n",
      "Epoch 4/64\n",
      "163/163 - 52s - 317ms/step - accuracy: 0.9872 - loss: 0.0362 - val_accuracy: 0.8173 - val_loss: 0.5799 - learning_rate: 2.0000e-04\n",
      "\n",
      "ðŸ”§ Training model: ResNet152V2\n",
      "Found 5216 images belonging to 2 classes.\n",
      "Found 624 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/64\n",
      "163/163 - 96s - 590ms/step - accuracy: 0.9358 - loss: 0.1630 - val_accuracy: 0.7436 - val_loss: 0.9833 - learning_rate: 0.0010\n",
      "Epoch 2/64\n",
      "163/163 - 51s - 311ms/step - accuracy: 0.9707 - loss: 0.0843 - val_accuracy: 0.8317 - val_loss: 0.5074 - learning_rate: 0.0010\n",
      "Epoch 3/64\n",
      "163/163 - 51s - 314ms/step - accuracy: 0.9689 - loss: 0.0819 - val_accuracy: 0.8670 - val_loss: 0.3838 - learning_rate: 0.0010\n",
      "Epoch 4/64\n",
      "163/163 - 51s - 313ms/step - accuracy: 0.9806 - loss: 0.0538 - val_accuracy: 0.8510 - val_loss: 0.4878 - learning_rate: 0.0010\n",
      "Epoch 5/64\n",
      "163/163 - 51s - 314ms/step - accuracy: 0.9827 - loss: 0.0483 - val_accuracy: 0.8510 - val_loss: 0.4972 - learning_rate: 0.0010\n",
      "Epoch 6/64\n",
      "163/163 - 51s - 311ms/step - accuracy: 0.9904 - loss: 0.0308 - val_accuracy: 0.7949 - val_loss: 0.8537 - learning_rate: 2.0000e-04\n",
      "\n",
      "ðŸ”§ Training model: InceptionV3\n",
      "Found 5216 images belonging to 2 classes.\n",
      "Found 624 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/64\n",
      "163/163 - 80s - 489ms/step - accuracy: 0.9210 - loss: 0.2012 - val_accuracy: 0.7580 - val_loss: 0.6993 - learning_rate: 0.0010\n",
      "Epoch 2/64\n",
      "163/163 - 49s - 302ms/step - accuracy: 0.9446 - loss: 0.1496 - val_accuracy: 0.8285 - val_loss: 0.4965 - learning_rate: 0.0010\n",
      "Epoch 3/64\n",
      "163/163 - 50s - 305ms/step - accuracy: 0.9565 - loss: 0.1175 - val_accuracy: 0.8670 - val_loss: 0.4481 - learning_rate: 0.0010\n",
      "Epoch 4/64\n",
      "163/163 - 49s - 299ms/step - accuracy: 0.9613 - loss: 0.0993 - val_accuracy: 0.7724 - val_loss: 0.7566 - learning_rate: 0.0010\n",
      "Epoch 5/64\n",
      "163/163 - 49s - 300ms/step - accuracy: 0.9594 - loss: 0.1067 - val_accuracy: 0.8093 - val_loss: 0.6377 - learning_rate: 0.0010\n",
      "Epoch 6/64\n",
      "163/163 - 50s - 306ms/step - accuracy: 0.9768 - loss: 0.0675 - val_accuracy: 0.8189 - val_loss: 0.6291 - learning_rate: 2.0000e-04\n",
      "\n",
      "ðŸ”§ Training model: InceptionResNetV2\n",
      "Found 5216 images belonging to 2 classes.\n",
      "Found 624 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/64\n",
      "163/163 - 106s - 651ms/step - accuracy: 0.8637 - loss: 0.3599 - val_accuracy: 0.7292 - val_loss: 0.7450 - learning_rate: 0.0010\n",
      "Epoch 2/64\n",
      "163/163 - 53s - 323ms/step - accuracy: 0.9387 - loss: 0.1558 - val_accuracy: 0.8109 - val_loss: 0.5282 - learning_rate: 0.0010\n",
      "Epoch 3/64\n",
      "163/163 - 51s - 314ms/step - accuracy: 0.9463 - loss: 0.1305 - val_accuracy: 0.7676 - val_loss: 0.7208 - learning_rate: 0.0010\n",
      "Epoch 4/64\n",
      "163/163 - 52s - 319ms/step - accuracy: 0.9551 - loss: 0.1186 - val_accuracy: 0.8269 - val_loss: 0.5177 - learning_rate: 0.0010\n",
      "Epoch 5/64\n",
      "163/163 - 52s - 318ms/step - accuracy: 0.9572 - loss: 0.1098 - val_accuracy: 0.8606 - val_loss: 0.4234 - learning_rate: 0.0010\n",
      "Epoch 6/64\n",
      "163/163 - 51s - 315ms/step - accuracy: 0.9553 - loss: 0.1142 - val_accuracy: 0.8317 - val_loss: 0.5211 - learning_rate: 0.0010\n",
      "Epoch 7/64\n",
      "163/163 - 52s - 319ms/step - accuracy: 0.9638 - loss: 0.0918 - val_accuracy: 0.7676 - val_loss: 0.8435 - learning_rate: 0.0010\n",
      "Epoch 8/64\n",
      "163/163 - 52s - 319ms/step - accuracy: 0.9722 - loss: 0.0764 - val_accuracy: 0.8269 - val_loss: 0.5338 - learning_rate: 2.0000e-04\n",
      "\n",
      "ðŸ”§ Training model: MobileNet\n",
      "Found 5216 images belonging to 2 classes.\n",
      "Found 624 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/64\n",
      "163/163 - 60s - 371ms/step - accuracy: 0.9519 - loss: 0.1256 - val_accuracy: 0.8654 - val_loss: 0.3553 - learning_rate: 0.0010\n",
      "Epoch 2/64\n",
      "163/163 - 50s - 309ms/step - accuracy: 0.9720 - loss: 0.0744 - val_accuracy: 0.8638 - val_loss: 0.3953 - learning_rate: 0.0010\n",
      "Epoch 3/64\n",
      "163/163 - 49s - 303ms/step - accuracy: 0.9793 - loss: 0.0556 - val_accuracy: 0.6955 - val_loss: 1.4100 - learning_rate: 0.0010\n",
      "Epoch 4/64\n",
      "163/163 - 49s - 303ms/step - accuracy: 0.9881 - loss: 0.0362 - val_accuracy: 0.8093 - val_loss: 0.6459 - learning_rate: 2.0000e-04\n",
      "\n",
      "ðŸ”§ Training model: MobileNetV2\n",
      "Found 5216 images belonging to 2 classes.\n",
      "Found 624 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/64\n",
      "163/163 - 68s - 415ms/step - accuracy: 0.9513 - loss: 0.1333 - val_accuracy: 0.8269 - val_loss: 0.5256 - learning_rate: 0.0010\n",
      "Epoch 2/64\n",
      "163/163 - 50s - 307ms/step - accuracy: 0.9789 - loss: 0.0595 - val_accuracy: 0.8462 - val_loss: 0.4604 - learning_rate: 0.0010\n",
      "Epoch 3/64\n",
      "163/163 - 50s - 304ms/step - accuracy: 0.9829 - loss: 0.0479 - val_accuracy: 0.7869 - val_loss: 0.8633 - learning_rate: 0.0010\n",
      "Epoch 4/64\n",
      "163/163 - 49s - 302ms/step - accuracy: 0.9873 - loss: 0.0375 - val_accuracy: 0.8301 - val_loss: 0.6677 - learning_rate: 0.0010\n",
      "Epoch 5/64\n",
      "163/163 - 50s - 304ms/step - accuracy: 0.9921 - loss: 0.0240 - val_accuracy: 0.7821 - val_loss: 0.9742 - learning_rate: 2.0000e-04\n",
      "\n",
      "ðŸ”§ Training model: DenseNet121\n",
      "Found 5216 images belonging to 2 classes.\n",
      "Found 624 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/64\n",
      "163/163 - 106s - 652ms/step - accuracy: 0.9252 - loss: 0.1882 - val_accuracy: 0.8558 - val_loss: 0.3306 - learning_rate: 0.0010\n",
      "Epoch 2/64\n",
      "163/163 - 52s - 320ms/step - accuracy: 0.9676 - loss: 0.0889 - val_accuracy: 0.7596 - val_loss: 0.7338 - learning_rate: 0.0010\n",
      "Epoch 3/64\n",
      "163/163 - 52s - 320ms/step - accuracy: 0.9739 - loss: 0.0739 - val_accuracy: 0.8237 - val_loss: 0.5168 - learning_rate: 0.0010\n",
      "Epoch 4/64\n",
      "163/163 - 53s - 322ms/step - accuracy: 0.9789 - loss: 0.0579 - val_accuracy: 0.7933 - val_loss: 0.6639 - learning_rate: 2.0000e-04\n",
      "\n",
      "ðŸ”§ Training model: DenseNet169\n",
      "Found 5216 images belonging to 2 classes.\n",
      "Found 624 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/64\n",
      "163/163 - 125s - 769ms/step - accuracy: 0.9396 - loss: 0.1548 - val_accuracy: 0.8189 - val_loss: 0.4318 - learning_rate: 0.0010\n",
      "Epoch 2/64\n",
      "163/163 - 54s - 331ms/step - accuracy: 0.9737 - loss: 0.0726 - val_accuracy: 0.8510 - val_loss: 0.4265 - learning_rate: 0.0010\n",
      "Epoch 3/64\n",
      "163/163 - 52s - 322ms/step - accuracy: 0.9806 - loss: 0.0562 - val_accuracy: 0.8478 - val_loss: 0.4458 - learning_rate: 0.0010\n",
      "Epoch 4/64\n",
      "163/163 - 53s - 322ms/step - accuracy: 0.9806 - loss: 0.0506 - val_accuracy: 0.7644 - val_loss: 0.9168 - learning_rate: 0.0010\n",
      "Epoch 5/64\n",
      "163/163 - 54s - 332ms/step - accuracy: 0.9875 - loss: 0.0355 - val_accuracy: 0.8013 - val_loss: 0.7116 - learning_rate: 2.0000e-04\n",
      "\n",
      "ðŸ”§ Training model: DenseNet201\n",
      "Found 5216 images belonging to 2 classes.\n",
      "Found 624 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/64\n",
      "163/163 - 142s - 869ms/step - accuracy: 0.9521 - loss: 0.1238 - val_accuracy: 0.8574 - val_loss: 0.4089 - learning_rate: 0.0010\n",
      "Epoch 2/64\n",
      "163/163 - 53s - 325ms/step - accuracy: 0.9682 - loss: 0.0766 - val_accuracy: 0.7901 - val_loss: 0.7076 - learning_rate: 0.0010\n",
      "Epoch 3/64\n",
      "163/163 - 53s - 326ms/step - accuracy: 0.9797 - loss: 0.0573 - val_accuracy: 0.8173 - val_loss: 0.5878 - learning_rate: 0.0010\n",
      "Epoch 4/64\n",
      "163/163 - 53s - 326ms/step - accuracy: 0.9877 - loss: 0.0363 - val_accuracy: 0.8381 - val_loss: 0.5312 - learning_rate: 2.0000e-04\n",
      "\n",
      "ðŸ”§ Training model: NASNetMobile\n",
      "Found 5216 images belonging to 2 classes.\n",
      "Found 624 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/64\n",
      "163/163 - 116s - 713ms/step - accuracy: 0.9294 - loss: 0.1700 - val_accuracy: 0.8077 - val_loss: 0.4794 - learning_rate: 0.0010\n",
      "Epoch 2/64\n",
      "163/163 - 51s - 311ms/step - accuracy: 0.9565 - loss: 0.1068 - val_accuracy: 0.7788 - val_loss: 0.6543 - learning_rate: 0.0010\n",
      "Epoch 3/64\n",
      "163/163 - 51s - 312ms/step - accuracy: 0.9657 - loss: 0.0896 - val_accuracy: 0.8429 - val_loss: 0.4233 - learning_rate: 0.0010\n",
      "Epoch 4/64\n",
      "163/163 - 50s - 309ms/step - accuracy: 0.9588 - loss: 0.0969 - val_accuracy: 0.8446 - val_loss: 0.4435 - learning_rate: 0.0010\n",
      "Epoch 5/64\n",
      "163/163 - 51s - 313ms/step - accuracy: 0.9682 - loss: 0.0812 - val_accuracy: 0.8654 - val_loss: 0.3951 - learning_rate: 0.0010\n",
      "Epoch 6/64\n",
      "163/163 - 52s - 318ms/step - accuracy: 0.9749 - loss: 0.0676 - val_accuracy: 0.8109 - val_loss: 0.6357 - learning_rate: 0.0010\n",
      "Epoch 7/64\n",
      "163/163 - 51s - 310ms/step - accuracy: 0.9747 - loss: 0.0661 - val_accuracy: 0.8237 - val_loss: 0.6377 - learning_rate: 0.0010\n",
      "Epoch 8/64\n",
      "163/163 - 51s - 312ms/step - accuracy: 0.9835 - loss: 0.0474 - val_accuracy: 0.8221 - val_loss: 0.6551 - learning_rate: 2.0000e-04\n",
      "âœ… Saved summary to /kaggle/working/classification_summary.csv\n",
      "ðŸ“Š Classification Summary:\n",
      "                Model  Accuracy  Macro Precision  Macro Recall  Macro F1  \\\n",
      "0            Xception  0.806090         0.863906      0.745726  0.762631   \n",
      "1               VGG16  0.854167         0.883497      0.813248  0.831469   \n",
      "2               VGG19  0.810897         0.869871      0.751282  0.768903   \n",
      "3            ResNet50  0.847756         0.875126      0.806410  0.824061   \n",
      "4          ResNet50V2  0.868590         0.903448      0.828205  0.847930   \n",
      "5           ResNet101  0.806090         0.857753      0.747436  0.764203   \n",
      "6         ResNet101V2  0.850962         0.875401      0.811538  0.828688   \n",
      "7           ResNet152  0.879808         0.885043      0.856838  0.867383   \n",
      "8         ResNet152V2  0.866987         0.884977      0.833761  0.849445   \n",
      "9         InceptionV3  0.866987         0.883366      0.834615  0.849815   \n",
      "10  InceptionResNetV2  0.860577         0.872588      0.829487  0.843337   \n",
      "11          MobileNet  0.865385         0.894963      0.826496  0.845061   \n",
      "12        MobileNetV2  0.846154         0.887520      0.799145  0.818918   \n",
      "13        DenseNet121  0.855769         0.880623      0.817094  0.834434   \n",
      "14        DenseNet169  0.850962         0.885735      0.807265  0.826330   \n",
      "15        DenseNet201  0.857372         0.885625      0.817521  0.835618   \n",
      "16       NASNetMobile  0.865385         0.885517      0.830769  0.847059   \n",
      "\n",
      "    Weighted Precision  Weighted Recall  Weighted F1  \n",
      "0             0.840046         0.806090     0.788023  \n",
      "1             0.868346         0.854167     0.846931  \n",
      "2             0.845403         0.810897     0.793531  \n",
      "3             0.860988         0.847756     0.840203  \n",
      "4             0.885560         0.868590     0.861943  \n",
      "5             0.835971         0.806090     0.789048  \n",
      "6             0.862433         0.850962     0.844131  \n",
      "7             0.881429         0.879808     0.877531  \n",
      "8             0.874583         0.866987     0.862293  \n",
      "9             0.873760         0.866987     0.862511  \n",
      "10            0.865326         0.860577     0.856329  \n",
      "11            0.879374         0.865385     0.859090  \n",
      "12            0.867846         0.846154     0.836475  \n",
      "13            0.867345         0.855769     0.849292  \n",
      "14            0.868459         0.850962     0.842681  \n",
      "15            0.870835         0.857372     0.850568  \n",
      "16            0.874138         0.865385     0.860294  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import (\n",
    "    VGG16, VGG19, ResNet50, ResNet50V2, ResNet101, ResNet101V2, ResNet152, ResNet152V2,\n",
    "    InceptionV3, InceptionResNetV2, Xception,\n",
    "    MobileNet, MobileNetV2,\n",
    "    DenseNet121, DenseNet169, DenseNet201,\n",
    "    NASNetMobile\n",
    ")\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input as vgg_preprocess\n",
    "from tensorflow.keras.applications.vgg19 import preprocess_input as vgg19_preprocess\n",
    "from tensorflow.keras.applications.resnet import preprocess_input as resnet_preprocess\n",
    "from tensorflow.keras.applications.resnet_v2 import preprocess_input as resnetv2_preprocess\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input as inception_preprocess\n",
    "from tensorflow.keras.applications.inception_resnet_v2 import preprocess_input as inceptionresnet_preprocess\n",
    "from tensorflow.keras.applications.xception import preprocess_input as xception_preprocess\n",
    "from tensorflow.keras.applications.mobilenet import preprocess_input as mobilenet_preprocess\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input as mobilenetv2_preprocess\n",
    "from tensorflow.keras.applications.densenet import preprocess_input as densenet_preprocess\n",
    "from tensorflow.keras.applications.nasnet import preprocess_input as nasnet_preprocess\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Constants\n",
    "train_dir = '/kaggle/input/chest-xray-pneumonia/chest_xray/train'\n",
    "test_dir = '/kaggle/input/chest-xray-pneumonia/chest_xray/test'\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 64\n",
    "IMG_SIZE = (224, 224)\n",
    "\n",
    "# Data Generators (NO AUGMENTATION)\n",
    "def get_generators(preprocess_func):\n",
    "    train_gen = ImageDataGenerator(\n",
    "        preprocessing_function=preprocess_func\n",
    "    ).flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=IMG_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='binary',\n",
    "        seed=SEED\n",
    "    )\n",
    "\n",
    "    test_gen = ImageDataGenerator(\n",
    "        preprocessing_function=preprocess_func\n",
    "    ).flow_from_directory(\n",
    "        test_dir,\n",
    "        target_size=IMG_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='binary',\n",
    "        shuffle=False\n",
    "    )\n",
    "    return train_gen, test_gen\n",
    "\n",
    "# Build classifier head\n",
    "def build_model(base_model):\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    output = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs=base_model.input, outputs=output)\n",
    "    return model\n",
    "\n",
    "# Models Dictionary\n",
    "models = {\n",
    "    'Xception': (Xception(weights='imagenet', include_top=False, input_shape=(224,224,3)), xception_preprocess),\n",
    "    'VGG16': (VGG16(weights='imagenet', include_top=False, input_shape=(224,224,3)), vgg_preprocess),\n",
    "    'VGG19': (VGG19(weights='imagenet', include_top=False, input_shape=(224,224,3)), vgg19_preprocess),\n",
    "    'ResNet50': (ResNet50(weights='imagenet', include_top=False, input_shape=(224,224,3)), resnet_preprocess),\n",
    "    'ResNet50V2': (ResNet50V2(weights='imagenet', include_top=False, input_shape=(224,224,3)), resnetv2_preprocess),\n",
    "    'ResNet101': (ResNet101(weights='imagenet', include_top=False, input_shape=(224,224,3)), resnet_preprocess),\n",
    "    'ResNet101V2': (ResNet101V2(weights='imagenet', include_top=False, input_shape=(224,224,3)), resnetv2_preprocess),\n",
    "    'ResNet152': (ResNet152(weights='imagenet', include_top=False, input_shape=(224,224,3)), resnet_preprocess),\n",
    "    'ResNet152V2': (ResNet152V2(weights='imagenet', include_top=False, input_shape=(224,224,3)), resnetv2_preprocess),\n",
    "    'InceptionV3': (InceptionV3(weights='imagenet', include_top=False, input_shape=(224,224,3)), inception_preprocess),\n",
    "    'InceptionResNetV2': (InceptionResNetV2(weights='imagenet', include_top=False, input_shape=(224,224,3)), inceptionresnet_preprocess),\n",
    "    'MobileNet': (MobileNet(weights='imagenet', include_top=False, input_shape=(224,224,3)), mobilenet_preprocess),\n",
    "    'MobileNetV2': (MobileNetV2(weights='imagenet', include_top=False, input_shape=(224,224,3)), mobilenetv2_preprocess),\n",
    "    'DenseNet121': (DenseNet121(weights='imagenet', include_top=False, input_shape=(224,224,3)), densenet_preprocess),\n",
    "    'DenseNet169': (DenseNet169(weights='imagenet', include_top=False, input_shape=(224,224,3)), densenet_preprocess),\n",
    "    'DenseNet201': (DenseNet201(weights='imagenet', include_top=False, input_shape=(224,224,3)), densenet_preprocess),\n",
    "    'NASNetMobile': (NASNetMobile(weights='imagenet', include_top=False, input_shape=(224,224,3)), nasnet_preprocess),\n",
    "}\n",
    "\n",
    "# Callbacks\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=2, factor=0.2)\n",
    "\n",
    "# Result storage\n",
    "report_rows = []\n",
    "\n",
    "# Training and Evaluation Loop\n",
    "for model_name, (base_model, preprocess_func) in models.items():\n",
    "    print(f\"\\nðŸ”§ Training model: {model_name}\")\n",
    "    train_gen, test_gen = get_generators(preprocess_func)\n",
    "    model = build_model(base_model)\n",
    "\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model.fit(\n",
    "        train_gen,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=test_gen,\n",
    "        callbacks=[early_stop, reduce_lr],\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "    predictions = model.predict(test_gen, verbose=0)\n",
    "    predicted_labels = (predictions > 0.5).astype(int).ravel()\n",
    "    true_labels = test_gen.classes\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "\n",
    "    # Classification Report\n",
    "    report = classification_report(true_labels, predicted_labels, digits=6, output_dict=True)\n",
    "\n",
    "    row = {\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Macro Precision': report['macro avg']['precision'],\n",
    "        'Macro Recall': report['macro avg']['recall'],\n",
    "        'Macro F1': report['macro avg']['f1-score'],\n",
    "        'Weighted Precision': report['weighted avg']['precision'],\n",
    "        'Weighted Recall': report['weighted avg']['recall'],\n",
    "        'Weighted F1': report['weighted avg']['f1-score'],\n",
    "    }\n",
    "\n",
    "    report_rows.append(row)\n",
    "\n",
    "# Save to CSV\n",
    "df_report = pd.DataFrame(report_rows)\n",
    "df_report.to_csv('/kaggle/working/classification_summary.csv', index=False)\n",
    "print(\"âœ… Saved summary to /kaggle/working/classification_summary.csv\")\n",
    "print(\"ðŸ“Š Classification Summary:\")\n",
    "print(df_report)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 17810,
     "sourceId": 23812,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5781.340406,
   "end_time": "2025-07-20T11:47:51.045285",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-20T10:11:29.704879",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
